{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ueVScjAhnV7f"
      },
      "outputs": [],
      "source": [
        "#Import Libararies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as trans\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation for training datasets\n",
        "train_transformation = trans.Compose([\n",
        "    trans.Resize((227,227)),  # Adjust to match AlexNet input dimensions\n",
        "    trans.RandomHorizontalFlip(p=0.7),  # Augmenting data\n",
        "    trans.ToTensor(),\n",
        "    trans.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizing\n",
        "])"
      ],
      "metadata": {
        "id": "NrD6ItSgoShZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation for testing datasets\n",
        "test_transformation = trans.Compose([\n",
        "    trans.Resize((227,227)),\n",
        "    trans.ToTensor(),\n",
        "    trans.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "ynncgSDRzebs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)  # Ensure reproducibility\n",
        "full_train_dataset = CIFAR10(\"data/\", train=True, download=True, transform=train_transformation)\n",
        "test_dataset = CIFAR10(\"data/\", train=False, download=True, transform=test_transformation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcVRXtYGoW-w",
        "outputId": "c972e910-ae98-4041-92e8-018109b86110"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting training dataset into training and validation datasets\n",
        "validation_size = 10000\n",
        "training_size = len(full_train_dataset) - validation_size\n",
        "training_dataset, validation_dataset = random_split(full_train_dataset, [training_size, validation_size])"
      ],
      "metadata": {
        "id": "8bXvcc_Rzmnf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader setup for training, validation, and test datasets\n",
        "batch_size = 64\n",
        "training_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "testing_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "EDz5uk8yzprU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Alexnet Architechture\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.pool_layer = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.fully_connected_layers = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, 10),  # CIFAR-10 has 10 distinct classes.\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.pool_layer(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fully_connected_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "NB8yr18nzskg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting device to CUDA if available\n",
        "compute_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "net = AlexNet().to(compute_device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
        "learning_rate_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
      ],
      "metadata": {
        "id": "JzyBD2_-ob6S"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10  # Number of epochs can be adjusted"
      ],
      "metadata": {
        "id": "7dLeim84rFNc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    net.train()  # Switch to training mode\n",
        "    epoch_loss = 0.0\n",
        "    for images, labels in training_loader:\n",
        "        images, labels = images.to(compute_device), labels.to(compute_device)\n",
        "        optimizer.zero_grad()\n",
        "        predictions = net(images)\n",
        "        loss = loss_function(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    learning_rate_scheduler.step()  # Update learning rate\n",
        "    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(training_loader)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx4Chv03rKNK",
        "outputId": "1ae4b4a3-8bca-4fca-f198-1bf8cb4dc4e5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.5648120071411133\n",
            "Epoch 2, Loss: 1.102625904560089\n",
            "Epoch 3, Loss: 0.8775941960334778\n",
            "Epoch 4, Loss: 0.7367440643310547\n",
            "Epoch 5, Loss: 0.6420056324481964\n",
            "Epoch 6, Loss: 0.5668321094989777\n",
            "Epoch 7, Loss: 0.5030257425785065\n",
            "Epoch 8, Loss: 0.45459479126930236\n",
            "Epoch 9, Loss: 0.40965993921756744\n",
            "Epoch 10, Loss: 0.35877228055000304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()  # Switch to evaluation mode\n",
        "correct_predictions = 0\n",
        "total_images = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in testing_loader:\n",
        "        images, labels = images.to(compute_device), labels.to(compute_device)\n",
        "        output = net(images)\n",
        "        _, predicted_labels = torch.max(output.data, 1)\n",
        "        total_images += labels.size(0)\n",
        "        correct_predictions += (predicted_labels == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct_predictions / total_images\n",
        "print(f'Accuracy of the network on the 10000 test images: {accuracy:.2f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yWehmOdrN0h",
        "outputId": "4b4d3763-0bbd-4ebd-d054-7a81ae6bd996"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 82.34 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description of Dataset**\n",
        "\n",
        "The code provided implements a variant of the AlexNet architecture for the CIFAR-10 dataset. The CIFAR-10 dataset is a widely-used dataset for benchmarking image recognition algorithms. It consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The dataset is split into 50,000 training images and 10,000 test images. The classes include airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.\n",
        "\n",
        "**Dataset and Transformations**\n",
        "\n",
        "1. The CIFAR-10 dataset is transformed to fit the input size of the AlexNet architecture (227x227 pixels) and normalized using mean and standard deviation values common for pre-trained models on ImageNet. This normalization helps in faster convergence and improves model performance.\n",
        "2. Data augmentation through random horizontal flipping is used to increase the diversity of the training set, reducing overfitting and improving the model's generalization.\n",
        "\n",
        "**Training and Test Results**\n",
        "\n",
        "1. The modified AlexNet model is trained for 10 epochs, showing a consistent decrease in loss, indicating that the model is learning effectively from the training dataset.\n",
        "2. The final accuracy on the test dataset is reported as 82.34%, which is a strong result for CIFAR-10, considering the simplicity of the approach and the architectural limitations of AlexNet compared to more recent deep learning models.\n",
        "\n",
        "\n",
        "**Observations and Improvements**\n",
        "\n",
        "1. The performance of the model could potentially be improved by incorporating more sophisticated data augmentation techniques, using more complex architectures like ResNet or DenseNet, or applying regularization techniques like dropout more extensively.\n",
        "2. Learning rate scheduling and fine-tuning of hyperparameters (such as the learning rate, batch size, or optimizer choice) could further enhance model accuracy."
      ],
      "metadata": {
        "id": "MLCa9jxV6M6X"
      }
    }
  ]
}